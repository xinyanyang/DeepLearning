{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Name: Xinyan Yang \n",
    "\n",
    "### NetID: xinyany2\n",
    "\n",
    "##### Implement and train a neural network from scratch in Python for the MNIST dataset (no PyTorch). The neural network should be trained on the Training Set using stochastic gradient descent. It should achieve 97-98% accuracy on the Test Set.\n",
    "\n",
    "*For full credit, submit via Compass (1) the code and (2) a paragraph (in a PDF document) which states the Test Accuracy and briefly describes the implementation. Due September 7 at 5:00 PM.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code implements the backpropogation algorithm to build a fully-connected network with a single hidden layer. It is composed of a forward step and a backward step. In the forward step, the output f(X;θ) and intermediary network values (Z,H,and U) are calculated. I choose Rectified linear units (ReLU) for the nonlinearities σ(z). In the backward step, the gradient with respect to the parameters θ is calculated. The backward step relies upon the values calculated in the forward step.\n",
    "\n",
    "Using 100 hidden units, **the test accuracy is 98%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding: utf-8\n",
    "import numpy as np\n",
    "import h5py\n",
    "import time\n",
    "import copy\n",
    "from random import randint\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load MNIST data\n",
    "MNIST_data = h5py.File('MNISTdata.hdf5', 'r')\n",
    "x_train = np.float32(MNIST_data['x_train'][:] )\n",
    "y_train = np.int32(np.array(MNIST_data['y_train'][:,0]))\n",
    "x_test = np.float32( MNIST_data['x_test'][:] )\n",
    "y_test = np.int32( np.array( MNIST_data['y_test'][:,0] ) )\n",
    "MNIST_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation of stochastic gradient descent algorithm\n",
    "#number of inputs\n",
    "num_inputs = 28*28\n",
    "#number of outputs\n",
    "num_outputs = 10\n",
    "num_hidden = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize parameters W, C, b_1, b_2\n",
    "model = {}\n",
    "model['W'] = np.random.randn(num_hidden, num_inputs) / np.sqrt(num_inputs)\n",
    "model['C'] = np.random.randn(num_outputs, num_hidden) / np.sqrt(num_inputs)\n",
    "model['b_1'] = np.random.randn(num_hidden)\n",
    "model['b_2'] = np.random.randn(num_outputs)\n",
    "model_grads = copy.deepcopy(model)\n",
    "\n",
    "def softmax_function(z):\n",
    "    ZZ = np.exp(z)/np.sum(np.exp(z))\n",
    "    return ZZ\n",
    "\n",
    "def ReLU(x):\n",
    "    \"\"\"\n",
    "    Implement the RELU function\n",
    "    \"\"\"\n",
    "    return x * (x > 0)\n",
    "\n",
    "def e(y, num_outputs=10):\n",
    "    \"\"\"\n",
    "    e(y) function\n",
    "    \"\"\"\n",
    "    ret = np.zeros(num_outputs, dtype=np.int)\n",
    "    ret[y] = 1.0\n",
    "    return ret\n",
    "\n",
    "def dReLU(z):\n",
    "    \"\"\"\n",
    "    derivative of ReLU function\n",
    "    \"\"\"\n",
    "    # To avoid bug when the overflow happens\n",
    "    if z - 0.0 == 0:\n",
    "        z = z + 1e-8\n",
    "    if z < 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "def forward(x,y,model):\n",
    "    Z = np.dot(model['W'],x) + model['b_1']\n",
    "    H = np.array([ReLU(z) for z in Z])\n",
    "    #if using sigmoid \n",
    "    #H = 1/(np.exp(-Z) + 1)\n",
    "    U = np.dot(model['C'], H) + model['b_2']\n",
    "    p = softmax_function(U)\n",
    "    return Z,H,p\n",
    "\n",
    "def backward(x, y, p, H, Z, model, model_grads):\n",
    "    dU = -e(y) + p\n",
    "    rou_b_2 = dU\n",
    "    rou_C = np.dot(dU.reshape(num_outputs, 1), H.reshape(1, num_hidden))\n",
    "    delta = np.dot(np.transpose(model['C']), dU)\n",
    "    rou_b_1 = delta * [dReLU(z) for z in Z]\n",
    "    #deri_sig = np.multiply(H,(1-H))     if using sigmoid\n",
    "    rou_w = np.dot(rou_b_1.reshape(len(rou_b_1), 1), np.transpose(x.reshape(len(x), 1)))\n",
    "        \n",
    "    model_grads['C'] = rou_C\n",
    "    model_grads['b_2'] = rou_b_2\n",
    "    model_grads['W'] = rou_w\n",
    "    model_grads['b_1'] = rou_b_1\n",
    "    assert model_grads['C'].shape == rou_C.shape\n",
    "    assert model_grads['b_2'].shape == rou_b_2.shape\n",
    "    assert model_grads['W'].shape == rou_w.shape\n",
    "    assert model_grads['b_1'].shape == rou_b_1.shape\n",
    "    return model_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0  | Training Accuracy: 0.9241\n",
      "epochs: 1  | Training Accuracy: 0.9655333333333334\n",
      "epochs: 2  | Training Accuracy: 0.9741\n",
      "epochs: 3  | Training Accuracy: 0.9779\n",
      "epochs: 4  | Training Accuracy: 0.9833833333333334\n",
      "epochs: 5  | Training Accuracy: 0.9842166666666666\n",
      "epochs: 6  | Training Accuracy: 0.9917166666666667\n",
      "epochs: 7  | Training Accuracy: 0.9937833333333334\n",
      "epochs: 8  | Training Accuracy: 0.9942666666666666\n",
      "epochs: 9  | Training Accuracy: 0.9943833333333333\n",
      "epochs: 10  | Training Accuracy: 0.9956\n",
      "epochs: 11  | Training Accuracy: 0.9964833333333334\n"
     ]
    }
   ],
   "source": [
    "LR = 0.01\n",
    "num_epochs = 12\n",
    "for epochs in range(num_epochs):\n",
    "    #Learning rate schedule\n",
    "    if (epochs > 5):\n",
    "        LR = 0.001\n",
    "    if (epochs > 10):\n",
    "        LR = 0.0001\n",
    "    if (epochs > 15):\n",
    "        LR = 0.00001\n",
    "    total_correct = 0\n",
    "    for n in range(len(x_train)):\n",
    "        n_random = randint(0,len(x_train)-1 )\n",
    "        y = y_train[n_random]\n",
    "        x = x_train[n_random][:]\n",
    "        Z,H,p = forward(x, y, model)\n",
    "        prediction = np.argmax(p)\n",
    "        if (prediction == y):\n",
    "            total_correct += 1\n",
    "        model_grads = backward(x, y, p, H, Z, model, model_grads)\n",
    "        #update parameters\n",
    "        model['C'] = model['C'] - LR*model_grads['C']\n",
    "        model['b_2'] = model['b_2'] - LR*model_grads['b_2']\n",
    "        model['b_1'] = model['b_1'] - LR*model_grads['b_1']\n",
    "        model['W'] = model['W'] - LR*model_grads['W']\n",
    "        \n",
    "    print('epochs: '+ str(epochs), ' | Training Accuracy: ' + str(total_correct/np.float(len(x_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "#test data\n",
    "total_correct = 0\n",
    "for n in range( len(x_test)):\n",
    "    y = y_test[n]\n",
    "    x = x_test[n][:]\n",
    "    Z,H,p = forward(x, y, model)\n",
    "    prediction = np.argmax(p)\n",
    "    if (prediction == y):\n",
    "        total_correct += 1\n",
    "print('Test Accuracy:', total_correct/np.float(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
